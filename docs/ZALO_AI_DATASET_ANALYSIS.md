# Zalo AI Traffic Dataset Analysis & Tailored Implementation Plan

## ğŸ“Š Dataset Overview

### Dataset Statistics
- **Training Set**: 1,490 samples
- **Public Test Set**: 405 samples
- **Language**: Vietnamese (Tiáº¿ng Viá»‡t)
- **Format**: Video-based multiple choice questions
- **Video Count**: ~550 unique videos (based on video file count)

### Data Structure

#### Training Data Format
```json
{
    "id": "train_0001",
    "question": "Náº¿u xe Ã´ tÃ´ Ä‘ang cháº¡y á»Ÿ lÃ n ngoÃ i cÃ¹ng bÃªn pháº£i trong video nÃ y thÃ¬ xe Ä‘Ã³ chá»‰ Ä‘Æ°á»£c phÃ©p ráº½ pháº£i?",
    "choices": [ 
        "A. ÄÃºng",
        "B. Sai"
    ],
    "answer": "B. Sai",
    "support_frames": [4.427402],
    "video_path": "train/videos/2b840c67_386_clip_002_0008_0018_Y.mp4"
}
```

#### Public Test Data Format
```json
{
    "id": "testa_0001",
    "question": "Theo trong video, náº¿u Ã´ tÃ´ Ä‘i hÆ°á»›ng cháº¿ch sang pháº£i lÃ  hÆ°á»›ng vÃ o Ä‘Æ°á»ng nÃ o?",
    "choices": [
        "A. KhÃ´ng cÃ³ thÃ´ng tin",
        "B. Dáº§u GiÃ¢y Long ThÃ nh",
        "C. ÄÆ°á»ng Äá»— XuÃ¢n Há»£p",
        "D. Xa Lá»™ HÃ  Ná»™i"
    ],
    "video_path": "public_test/videos/efc9909e_908_clip_001_0000_0009_Y.mp4"
}
```

**Key Difference**: Training data has `answer` and `support_frames`, test data doesn't.

### Question Types Analysis

Based on the sample questions, your dataset includes:

1. **Lane/Direction Questions**
   - "LÃ n Ä‘Æ°á»ng xe Ä‘ang cháº¡y cÃ³ Ä‘Æ°á»£c phÃ©p ráº½ trÃ¡i hay khÃ´ng?"
   - "Náº¿u xe Ã´ tÃ´ Ä‘ang cháº¡y á»Ÿ lÃ n ngoÃ i cÃ¹ng bÃªn pháº£i..."

2. **Traffic Sign Recognition**
   - "Biá»ƒn chá»‰ dáº«n 3 hÆ°á»›ng di chuyá»ƒn chÃ­nh tiáº¿p theo, Ä‘Ãºng hay sai?"
   - "Theo biá»ƒn bÃ¡o trong video, muá»‘n Ä‘i Ä‘áº¿n Ä‘Æ°á»ng LÆ°Æ¡ng ÄÃ¬nh Cá»§a thÃ¬ pháº£i Ä‘i hÆ°á»›ng nÃ o?"

3. **Navigation/Route Questions**
   - "Muá»‘n Ä‘i Cáº§u Ráº¡ch Chiáº¿c thÃ¬ cáº§n ráº½ pháº£i, Ä‘Ãºng hay sai?"
   - Street names and direction questions

4. **True/False Questions**
   - Binary choice (ÄÃºng/Sai)

5. **Multiple Choice Questions**
   - 2-4 options (A, B, C, D)

### Video Characteristics

**Naming Convention**: `{hash}_{id}_clip_{clip_num}_{start}_{end}_{label}.mp4`
- Example: `2b840c67_386_clip_002_0008_0018_Y.mp4`
- `Y` likely indicates positive/yes label
- `N` likely indicates negative/no label

**Support Frames**:
- Timestamps indicating key frames for answering questions
- Single timestamp per question in training data

---

## ğŸ¯ Tailored Implementation Plan for Zalo AI Dataset

### Phase 1: Data Preparation & Understanding (Week 1)

#### 1.1 Dataset Exploration
```python
# Create data exploration notebook
import json
import cv2
from collections import Counter

# Load and analyze data
with open('train/train.json', 'r', encoding='utf-8') as f:
    train_data = json.load(f)

# Statistics to gather:
# - Question length distribution
# - Answer distribution (A, B, C, D)
# - Video duration analysis
# - Support frame timestamp patterns
# - Question type categorization
```

#### 1.2 Video Processing Pipeline
```python
# Video preprocessing for Vietnamese traffic videos
def process_video(video_path, support_frames=None):
    """
    Extract frames from video, focus on support frames

    Args:
        video_path: Path to video file
        support_frames: List of timestamps (in seconds)

    Returns:
        frames: List of key frames as numpy arrays
    """
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)

    frames = []

    if support_frames:
        # Extract frames at specific timestamps
        for timestamp in support_frames:
            frame_number = int(timestamp * fps)
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)
            ret, frame = cap.read()
            if ret:
                frames.append(frame)
    else:
        # Uniform sampling (e.g., 8 frames)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        frame_indices = np.linspace(0, total_frames-1, 8, dtype=int)

        for idx in frame_indices:
            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
            ret, frame = cap.read()
            if ret:
                frames.append(frame)

    cap.release()
    return frames
```

#### 1.3 Vietnamese Language Considerations
```python
# Install Vietnamese language support
# pip install underthesea  # Vietnamese NLP toolkit
# pip install pyvi  # Vietnamese word segmentation

from underthesea import word_tokenize

# Example Vietnamese text processing
def preprocess_vietnamese_text(text):
    """
    Preprocess Vietnamese traffic questions
    """
    # Word tokenization for Vietnamese
    tokens = word_tokenize(text, format="text")
    return tokens
```

### Phase 2: Model Selection & Setup (Week 2)

#### 2.1 Base Model Options

**Option 1: Qwen2-VL (Recommended for Vietnamese)**
```bash
# Qwen2-VL has good multilingual support including Vietnamese
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor

model_name = "Qwen/Qwen2-VL-7B-Instruct"
model = Qwen2VLForConditionalGeneration.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
processor = AutoProcessor.from_pretrained(model_name)
```

**Option 2: ViLT + Vietnamese LLM**
```python
# Combine vision model with Vietnamese language model
# ViLT for vision + PhoBERT/viBERT for Vietnamese language understanding
```

#### 2.2 Data Formatting for Training
```python
def format_training_sample(sample):
    """
    Format Zalo AI sample for model training
    """
    question = sample['question']
    choices = '\n'.join(sample['choices'])
    answer = sample['answer']

    # Create instruction format
    instruction = f"""Báº¡n lÃ  má»™t trá»£ lÃ½ AI chuyÃªn vá» giao thÃ´ng Viá»‡t Nam.
HÃ£y xem video vÃ  tráº£ lá»i cÃ¢u há»i sau:

CÃ¢u há»i: {question}

CÃ¡c lá»±a chá»n:
{choices}

HÃ£y phÃ¢n tÃ­ch tá»«ng bÆ°á»›c vÃ  chá»n Ä‘Ã¡p Ã¡n Ä‘Ãºng."""

    response = f"""PhÃ¢n tÃ­ch:
[MÃ´ táº£ cÃ¡c yáº¿u tá»‘ quan trá»ng trong video]

ÄÃ¡p Ã¡n: {answer}"""

    return {
        'instruction': instruction,
        'video_path': sample['video_path'],
        'support_frames': sample.get('support_frames', []),
        'response': response
    }
```

### Phase 3: Fine-tuning Strategy (Weeks 3-4)

#### 3.1 LoRA Configuration for Vietnamese Traffic
```python
from peft import LoraConfig, get_peft_model

# LoRA configuration optimized for your dataset
lora_config = LoraConfig(
    r=16,                      # Rank
    lora_alpha=32,             # Scaling factor
    target_modules=[           # Target attention layers
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj"
    ],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

# Apply LoRA to model
model = get_peft_model(base_model, lora_config)
```

#### 3.2 Training Configuration
```python
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./traffic-mllm-vietnamese",
    num_train_epochs=5,                    # Adjusted for 1,490 samples
    per_device_train_batch_size=2,         # Small batch for video
    gradient_accumulation_steps=8,         # Effective batch = 16
    learning_rate=2e-4,
    warmup_steps=100,
    logging_steps=10,
    save_steps=100,
    eval_steps=100,
    evaluation_strategy="steps",
    save_total_limit=3,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    fp16=True,                             # Mixed precision training
    dataloader_num_workers=4,
    remove_unused_columns=False,
)
```

#### 3.3 Custom Dataset Class
```python
import torch
from torch.utils.data import Dataset
import cv2
import numpy as np

class ZaloTrafficDataset(Dataset):
    def __init__(self, json_path, video_root, processor, max_frames=8):
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        self.samples = data['data']
        self.video_root = video_root
        self.processor = processor
        self.max_frames = max_frames

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]

        # Load video frames
        video_path = os.path.join(self.video_root, sample['video_path'])
        frames = self.extract_frames(
            video_path,
            sample.get('support_frames', [])
        )

        # Format instruction
        instruction = self.format_instruction(sample)

        # Process with model processor
        inputs = self.processor(
            text=instruction,
            images=frames,
            return_tensors="pt",
            padding=True,
            truncation=True
        )

        # Add labels if training data
        if 'answer' in sample:
            inputs['labels'] = self.processor(
                text=sample['answer'],
                return_tensors="pt",
                padding=True,
                truncation=True
            )['input_ids']

        return inputs

    def extract_frames(self, video_path, support_frames):
        """Extract frames from video"""
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

        frames = []

        if support_frames and len(support_frames) > 0:
            # Use support frames + context frames
            for timestamp in support_frames[:3]:  # Max 3 support frames
                frame_num = int(timestamp * fps)
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)
                ret, frame = cap.read()
                if ret:
                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    frames.append(frame)

            # Add context frames
            remaining = self.max_frames - len(frames)
            if remaining > 0:
                indices = np.linspace(0, total_frames-1, remaining, dtype=int)
                for idx in indices:
                    cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
                    ret, frame = cap.read()
                    if ret:
                        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                        frames.append(frame)
        else:
            # Uniform sampling
            indices = np.linspace(0, total_frames-1, self.max_frames, dtype=int)
            for idx in indices:
                cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
                ret, frame = cap.read()
                if ret:
                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    frames.append(frame)

        cap.release()
        return frames[:self.max_frames]

    def format_instruction(self, sample):
        """Format Vietnamese traffic question"""
        question = sample['question']
        choices = '\n'.join(sample['choices'])

        instruction = f"""Báº¡n lÃ  trá»£ lÃ½ AI chuyÃªn vá» giao thÃ´ng Viá»‡t Nam. Xem video vÃ  tráº£ lá»i cÃ¢u há»i.

CÃ¢u há»i: {question}

Lá»±a chá»n:
{choices}

Tráº£ lá»i:"""
        return instruction
```

### Phase 4: Vietnamese Traffic Knowledge Base (Week 5)

#### 4.1 Build Traffic Rules Knowledge Base
```python
# Create Vietnamese traffic rules knowledge base
vietnamese_traffic_rules = {
    "lane_rules": [
        "LÃ n Ä‘Æ°á»ng bÃªn pháº£i nháº¥t cÃ³ thá»ƒ Ä‘i tháº³ng hoáº·c ráº½ pháº£i",
        "LÃ n Ä‘Æ°á»ng giá»¯a chá»‰ Ä‘Æ°á»£c Ä‘i tháº³ng",
        "LÃ n Ä‘Æ°á»ng bÃªn trÃ¡i cÃ³ thá»ƒ Ä‘i tháº³ng hoáº·c ráº½ trÃ¡i",
        "KhÃ´ng Ä‘Æ°á»£c Ä‘á»•i lÃ n khi cÃ³ váº¡ch liá»n",
    ],
    "traffic_signs": [
        "Biá»ƒn bÃ¡o cáº¥m ráº½ trÃ¡i: khÃ´ng Ä‘Æ°á»£c ráº½ trÃ¡i",
        "Biá»ƒn bÃ¡o cáº¥m ráº½ pháº£i: khÃ´ng Ä‘Æ°á»£c ráº½ pháº£i",
        "Biá»ƒn chá»‰ dáº«n hÆ°á»›ng Ä‘i: chá»‰ cÃ¡c hÆ°á»›ng Ä‘Æ°á»£c phÃ©p",
        "Biá»ƒn cáº£nh bÃ¡o: cáº£nh bÃ¡o nguy hiá»ƒm phÃ­a trÆ°á»›c",
    ],
    "road_markings": [
        "Váº¡ch liá»n: khÃ´ng Ä‘Æ°á»£c vÆ°á»£t hoáº·c Ä‘á»•i lÃ n",
        "Váº¡ch Ä‘á»©t: Ä‘Æ°á»£c phÃ©p Ä‘á»•i lÃ n",
        "MÅ©i tÃªn chá»‰ hÆ°á»›ng: chá»‰ hÆ°á»›ng Ä‘i báº¯t buá»™c cá»§a lÃ n",
    ]
}

# Save as JSON for RAG
import json
with open('knowledge_base/vietnamese_traffic_rules.json', 'w', encoding='utf-8') as f:
    json.dump(vietnamese_traffic_rules, f, ensure_ascii=False, indent=2)
```

#### 4.2 RAG Implementation for Vietnamese
```python
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Use Vietnamese-friendly embeddings
embeddings = HuggingFaceEmbeddings(
    model_name="keepitreal/vietnamese-sbert",  # Vietnamese sentence embeddings
    model_kwargs={'device': 'cuda'}
)

# Load and process traffic rules
def build_knowledge_base():
    """Build Vietnamese traffic knowledge base"""

    # Load rules
    with open('knowledge_base/vietnamese_traffic_rules.json', 'r', encoding='utf-8') as f:
        rules = json.load(f)

    # Flatten rules into text documents
    documents = []
    for category, rule_list in rules.items():
        for rule in rule_list:
            documents.append(rule)

    # Create vector store
    vectorstore = FAISS.from_texts(
        documents,
        embeddings,
        metadatas=[{"category": cat} for cat in rules.keys() for _ in rules[cat]]
    )

    return vectorstore

# Retrieve relevant rules
def retrieve_rules(question, vectorstore, k=3):
    """Retrieve relevant traffic rules for question"""
    docs = vectorstore.similarity_search(question, k=k)
    return "\n".join([doc.page_content for doc in docs])
```

### Phase 5: Chain-of-Thought for Vietnamese (Week 6)

#### 5.1 Vietnamese CoT Prompt Template
```python
COT_TEMPLATE_VIETNAMESE = """Báº¡n lÃ  chuyÃªn gia giao thÃ´ng Viá»‡t Nam. PhÃ¢n tÃ­ch video vÃ  tráº£ lá»i theo cÃ¡c bÆ°á»›c:

Video: {video_description}
Quy táº¯c liÃªn quan: {retrieved_rules}

CÃ¢u há»i: {question}
Lá»±a chá»n: {choices}

HÃ£y suy nghÄ© tá»«ng bÆ°á»›c:

BÆ°á»›c 1 - PhÃ¢n tÃ­ch cáº£nh quay:
- MÃ´ táº£ nhá»¯ng gÃ¬ tháº¥y trong video
- XÃ¡c Ä‘á»‹nh cÃ¡c biá»ƒn bÃ¡o, lÃ n Ä‘Æ°á»ng, phÆ°Æ¡ng tiá»‡n

BÆ°á»›c 2 - Ãp dá»¥ng quy táº¯c giao thÃ´ng:
- Quy táº¯c nÃ o liÃªn quan Ä‘áº¿n tÃ¬nh huá»‘ng nÃ y?
- Quy Ä‘á»‹nh cá»§a luáº­t giao thÃ´ng Viá»‡t Nam

BÆ°á»›c 3 - ÄÆ°a ra káº¿t luáº­n:
- PhÃ¢n tÃ­ch tá»«ng lá»±a chá»n
- Chá»n Ä‘Ã¡p Ã¡n Ä‘Ãºng vÃ  giáº£i thÃ­ch

ÄÃ¡p Ã¡n cuá»‘i cÃ¹ng: [A/B/C/D]
"""

def format_cot_prompt(sample, retrieved_rules=""):
    """Format Chain-of-Thought prompt for Vietnamese"""
    return COT_TEMPLATE_VIETNAMESE.format(
        video_description="[MÃ´ táº£ tá»« model vision]",
        retrieved_rules=retrieved_rules,
        question=sample['question'],
        choices='\n'.join(sample['choices'])
    )
```

### Phase 6: Training & Evaluation (Weeks 7-8)

#### 6.1 Training Script
```python
# train.py
import torch
from transformers import Trainer
from peft import prepare_model_for_kbit_training

# Load base model
model = Qwen2VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2-VL-7B-Instruct",
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True
)

# Prepare for training
model = prepare_model_for_kbit_training(model)

# Apply LoRA
model = get_peft_model(model, lora_config)

# Load datasets
train_dataset = ZaloTrafficDataset(
    'train/train.json',
    'train/videos',
    processor
)

# 80-20 split for train/val
from sklearn.model_selection import train_test_split
train_indices, val_indices = train_test_split(
    range(len(train_dataset)),
    test_size=0.2,
    random_state=42
)

train_subset = torch.utils.data.Subset(train_dataset, train_indices)
val_subset = torch.utils.data.Subset(train_dataset, val_indices)

# Define metrics
def compute_metrics(eval_pred):
    """Compute accuracy for multiple choice"""
    predictions, labels = eval_pred
    # Extract predicted answer (A, B, C, or D)
    # Compare with ground truth
    # Return accuracy
    pass

# Train
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_subset,
    eval_dataset=val_subset,
    compute_metrics=compute_metrics,
)

trainer.train()
```

#### 6.2 Evaluation on Public Test
```python
# evaluate.py
def evaluate_on_public_test(model, processor, test_json_path):
    """Evaluate model on Zalo AI public test set"""

    with open(test_json_path, 'r', encoding='utf-8') as f:
        test_data = json.load(f)

    predictions = []

    for sample in test_data['data']:
        # Load video
        frames = extract_frames(sample['video_path'])

        # Format prompt
        prompt = format_instruction(sample)

        # Generate prediction
        inputs = processor(text=prompt, images=frames, return_tensors="pt")
        outputs = model.generate(**inputs, max_new_tokens=100)
        answer = processor.decode(outputs[0], skip_special_tokens=True)

        # Extract choice (A, B, C, or D)
        predicted_choice = extract_choice(answer)

        predictions.append({
            'id': sample['id'],
            'predicted_answer': predicted_choice
        })

    # Save predictions
    save_predictions(predictions, 'predictions.json')

    return predictions
```

#### 6.3 Submission Format
```python
def create_submission(predictions):
    """Create submission file for Zalo AI"""
    submission = []

    for pred in predictions:
        submission.append({
            'id': pred['id'],
            'answer': pred['predicted_answer']  # e.g., "A", "B", "C", "D"
        })

    with open('submission.csv', 'w', encoding='utf-8') as f:
        f.write('id,answer\n')
        for item in submission:
            f.write(f"{item['id']},{item['answer']}\n")
```

---

## ğŸ“ Tailored Project Structure

```
zalo-traffic-mllm/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ model_config.yaml
â”‚   â””â”€â”€ training_config.yaml
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ train.json              # Your 1,490 training samples
â”‚   â”‚   â””â”€â”€ videos/                 # Training videos
â”‚   â”œâ”€â”€ public_test/
â”‚   â”‚   â”œâ”€â”€ public_test.json        # Your 405 test samples
â”‚   â”‚   â””â”€â”€ videos/                 # Test videos
â”‚   â””â”€â”€ knowledge_base/
â”‚       â””â”€â”€ vietnamese_traffic_rules.json
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ dataset.py              # ZaloTrafficDataset class
â”‚   â”‚   â”œâ”€â”€ video_processor.py      # Frame extraction
â”‚   â”‚   â””â”€â”€ vietnamese_processor.py # Vietnamese text processing
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ base_model.py           # Load Qwen2-VL
â”‚   â”‚   â”œâ”€â”€ lora_config.py          # LoRA setup
â”‚   â”‚   â””â”€â”€ traffic_mllm.py         # Main model
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ knowledge_base.py       # Vietnamese traffic rules
â”‚   â”‚   â””â”€â”€ retriever.py            # RAG implementation
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ trainer.py              # Training loop
â”‚   â”‚   â”œâ”€â”€ metrics.py              # Evaluation metrics
â”‚   â”‚   â””â”€â”€ utils.py                # Helper functions
â”‚   â””â”€â”€ inference/
â”‚       â”œâ”€â”€ predictor.py            # Prediction pipeline
â”‚       â””â”€â”€ submission.py           # Create submission file
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb   # Analyze your dataset
â”‚   â”œâ”€â”€ 02_video_analysis.ipynb     # Video processing tests
â”‚   â”œâ”€â”€ 03_model_testing.ipynb      # Test base model
â”‚   â””â”€â”€ 04_evaluation.ipynb         # Evaluate results
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.sh                    # Training script
â”‚   â”œâ”€â”€ evaluate.sh                 # Evaluation script
â”‚   â””â”€â”€ predict.sh                  # Inference script
â”œâ”€â”€ knowledge_base/
â”‚   â””â”€â”€ vietnamese_traffic_rules.json
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ¯ Simplified MVP for Your Dataset

### Option 1: Quick Start (2-3 weeks)

**Use Pre-trained Qwen2-VL with Few-Shot Learning**

```python
# No fine-tuning, just few-shot prompting
def few_shot_inference(model, processor, sample, examples):
    """
    Use few-shot learning with your training examples
    """

    # Format few-shot examples
    few_shot_prompt = "ÄÃ¢y lÃ  má»™t sá»‘ vÃ­ dá»¥:\n\n"

    for ex in examples[:3]:  # Use 3 examples
        few_shot_prompt += f"""
Video: [MÃ´ táº£ video]
CÃ¢u há»i: {ex['question']}
Lá»±a chá»n: {', '.join(ex['choices'])}
ÄÃ¡p Ã¡n: {ex['answer']}

"""

    # Add current question
    few_shot_prompt += f"""
BÃ¢y giá» hÃ£y tráº£ lá»i cÃ¢u há»i má»›i:
Video: [MÃ´ táº£ tá»« video hiá»‡n táº¡i]
CÃ¢u há»i: {sample['question']}
Lá»±a chá»n: {', '.join(sample['choices'])}
ÄÃ¡p Ã¡n:
"""

    # Generate answer
    # ... inference code
```

### Option 2: Fine-tuning Approach (6-8 weeks)

1. **Week 1-2**: Data preparation + exploration
2. **Week 3-4**: LoRA fine-tuning
3. **Week 5**: RAG knowledge base
4. **Week 6**: CoT integration
5. **Week 7-8**: Evaluation + optimization

---

## ğŸ’¡ Key Insights for Your Dataset

### 1. Support Frames are Gold
- Your training data has `support_frames` timestamps
- These indicate KEY moments for answering
- **Priority**: Extract frames at these timestamps first

### 2. Vietnamese Language Handling
- Use Vietnamese-friendly models (Qwen2-VL has good multilingual support)
- Consider Vietnamese embeddings for RAG
- Vietnamese traffic terminology is domain-specific

### 3. Multiple Choice Format
- Output needs to be exact: "A. ÄÃºng", "B. Sai", etc.
- Can simplify to just letter extraction: "A", "B", "C", "D"

### 4. Dataset Size Considerations
- 1,490 samples is moderate (not too small, not too large)
- Perfect for LoRA fine-tuning (don't need full fine-tuning)
- Can do 80-20 train/val split (1,192 train / 298 val)

---

## ğŸ“Š Expected Performance Targets

| Metric | Baseline | Target | Stretch Goal |
|--------|----------|--------|--------------|
| Accuracy | 40-50% | 70-75% | 80%+ |
| Training Time | - | 4-6 hours | 2-3 hours |
| Inference Speed | - | <3s/video | <1s/video |

---

## ğŸš€ Quick Start Commands

```bash
# 1. Setup environment
conda create -n zalo-traffic python=3.10
conda activate zalo-traffic
pip install -r requirements.txt

# 2. Explore data
python scripts/explore_data.py

# 3. Test base model
python scripts/test_base_model.py

# 4. Start training
python scripts/train.py --config config/training_config.yaml

# 5. Evaluate
python scripts/evaluate.py --checkpoint ./best_model

# 6. Create submission
python scripts/create_submission.py --output submission.csv
```

---

## ğŸ“ Next Steps

1. **Immediate**: Run data exploration notebook
2. **This Week**: Set up base model and test on few samples
3. **Next Week**: Start LoRA fine-tuning
4. **Following Weeks**: Add RAG + CoT, optimize performance

Would you like me to generate starter code for any of these components?
