{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "643f6cff",
   "metadata": {},
   "source": [
    "# Traffic Video MCQ Dataset Preparation\n",
    "\n",
    "This notebook prepares the Zalo AI Traffic dataset for fine-tuning VLMs.\n",
    "Dataset structure follows HuggingFaceM4/ChartQA format but adapted for multiple-choice questions with video frames.\n",
    "\n",
    "**Dataset Structure:**\n",
    "- Questions: Vietnamese traffic safety multiple-choice questions\n",
    "- Videos: Dashcam footage with support frames\n",
    "- Answers: A/B/C/D format with full text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05618a59",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66a193b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for Qwen2.5-VL with 4-bit quantization\n",
    "# !pip install datasets opencv-python pillow numpy tqdm\n",
    "# !pip install transformers accelerate bitsandbytes peft\n",
    "# !pip install qwen-vl-utils  # For Qwen2.5-VL video/image processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d884106e",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78070e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.0+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 5060 Ti\n",
      "CUDA version: 12.8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset, DatasetDict\n",
    "import torch\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check CUDA availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebd50dc",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a973292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Model: Qwen/Qwen2.5-VL-3B-Instruct\n",
      "üìä Train JSON: d:\\ZALO_AI\\trainining\\train\\train.json\n",
      "üìπ Videos Path: d:\\ZALO_AI\\trainining\\train\\videos\n",
      "üíæ Output Directory: d:\\ZALO_AI\\trainining\\processed_dataset\n",
      "‚öôÔ∏è 4-bit Quantization: True\n",
      "üñºÔ∏è Max frames per video: 8\n"
     ]
    }
   ],
   "source": [
    "# Dataset paths\n",
    "TRAIN_JSON_PATH = r\"d:\\ZALO_AI\\trainining\\train\\train.json\"\n",
    "VIDEOS_PATH = r\"d:\\ZALO_AI\\trainining\\train\\videos\"\n",
    "\n",
    "# Model configuration for Qwen2.5-VL\n",
    "MODEL_ID = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "\n",
    "# Processing parameters\n",
    "MAX_FRAMES = 8  # Maximum frames to extract per video (Qwen2.5-VL optimized)\n",
    "TRAIN_SPLIT = 0.9  # 90% for training, 10% for validation\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# 4-bit Quantization config\n",
    "USE_4BIT = True\n",
    "BNB_4BIT_COMPUTE_DTYPE = torch.bfloat16\n",
    "BNB_4BIT_QUANT_TYPE = \"nf4\"\n",
    "BNB_4BIT_USE_DOUBLE_QUANT = True\n",
    "\n",
    "# Output path\n",
    "OUTPUT_DIR = r\"d:\\ZALO_AI\\trainining\\processed_dataset\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"üéØ Model: {MODEL_ID}\")\n",
    "print(f\"üìä Train JSON: {TRAIN_JSON_PATH}\")\n",
    "print(f\"üìπ Videos Path: {VIDEOS_PATH}\")\n",
    "print(f\"üíæ Output Directory: {OUTPUT_DIR}\")\n",
    "print(f\"‚öôÔ∏è 4-bit Quantization: {USE_4BIT}\")\n",
    "print(f\"üñºÔ∏è Max frames per video: {MAX_FRAMES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd33002f",
   "metadata": {},
   "source": [
    "## 4. Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33eb176a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 1490 training samples\n",
      "\n",
      "üìä Sample structure:\n",
      "  id: train_0001\n",
      "  question: N·∫øu xe √¥ t√¥ ƒëang ch·∫°y ·ªü l√†n ngo√†i c√πng b√™n ph·∫£i trong video n√†y th√¨ xe ƒë√≥ ch·ªâ ƒë∆∞·ª£c ph√©p r·∫Ω ph·∫£i?\n",
      "  choices: ['A. ƒê√∫ng', 'B. Sai']\n",
      "  answer: B. Sai\n",
      "  support_frames: [4.427402]\n",
      "  video_path: train/videos/2b840c67_386_clip_002_0008_0018_Y.mp4\n"
     ]
    }
   ],
   "source": [
    "# Load train.json\n",
    "with open(TRAIN_JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "train_samples = raw_data['data']\n",
    "total_samples = len(train_samples)\n",
    "\n",
    "print(f\"‚úÖ Loaded {total_samples} training samples\")\n",
    "print(f\"\\nüìä Sample structure:\")\n",
    "sample = train_samples[0]\n",
    "for key, value in sample.items():\n",
    "    if key not in ['_unused_']:\n",
    "        print(f\"  {key}: {value if not isinstance(value, list) or len(str(value)) < 100 else str(value)[:100] + '...'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0e3c8b",
   "metadata": {},
   "source": [
    "## 5. Video Frame Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9f3cd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames_from_video(video_path, support_frames=None, max_frames=12):\n",
    "    \"\"\"\n",
    "    Extract frames from video at support frame timestamps or uniformly.\n",
    "\n",
    "    Args:\n",
    "        video_path: Path to video file\n",
    "        support_frames: List of timestamps (in seconds) to extract frames\n",
    "        max_frames: Maximum number of frames to extract\n",
    "\n",
    "    Returns:\n",
    "        List of PIL Images\n",
    "    \"\"\"\n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"‚ö†Ô∏è Video not found: {video_path}\")\n",
    "        return []\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"‚ö†Ô∏è Cannot open video: {video_path}\")\n",
    "        return []\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    duration = total_frames / fps if fps > 0 else 0\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    # Use support frames if provided, otherwise uniform sampling\n",
    "    if support_frames and len(support_frames) > 0:\n",
    "        # Extract frames at support timestamps\n",
    "        for timestamp in support_frames[:max_frames]:\n",
    "            frame_idx = int(timestamp * fps)\n",
    "            if frame_idx < total_frames:\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "                ret, frame = cap.read()\n",
    "                if ret:\n",
    "                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    frames.append(Image.fromarray(frame_rgb))\n",
    "\n",
    "        # If not enough frames from support_frames, add uniform samples\n",
    "        if len(frames) < max_frames and total_frames > len(frames):\n",
    "            remaining = max_frames - len(frames)\n",
    "            uniform_indices = np.linspace(0, total_frames - 1, remaining, dtype=int)\n",
    "            for idx in uniform_indices:\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "                ret, frame = cap.read()\n",
    "                if ret:\n",
    "                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    frames.append(Image.fromarray(frame_rgb))\n",
    "    else:\n",
    "        # Uniform sampling across the video\n",
    "        if duration < 10:  # Short videos: sample every 0.5s\n",
    "            frame_interval = max(1, int(fps * 0.5))\n",
    "            frame_indices = list(range(0, total_frames, frame_interval))[:max_frames]\n",
    "        else:\n",
    "            frame_indices = np.linspace(0, total_frames - 1, max_frames, dtype=int)\n",
    "\n",
    "        for frame_idx in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(Image.fromarray(frame_rgb))\n",
    "\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "\n",
    "def format_question_with_choices(question, choices):\n",
    "    \"\"\"\n",
    "    Format question with multiple choice options.\n",
    "    Similar to ChartQA 'query' field.\n",
    "    \"\"\"\n",
    "    formatted = f\"{question}\\n\\nC√°c l·ª±a ch·ªçn:\\n\"\n",
    "    for choice in choices:\n",
    "        formatted += f\"{choice}\\n\"\n",
    "    formatted += \"\\nH√£y ch·ªçn ƒë√°p √°n ƒë√∫ng:\"\n",
    "    return formatted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c29679",
   "metadata": {},
   "source": [
    "## 6. Data Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b13cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processing functions defined\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def process_sample(sample, videos_base_path, max_frames=12):\n",
    "    \"\"\"\n",
    "    Process a single sample into ChartQA-like format.\n",
    "\n",
    "    Returns:\n",
    "        dict with keys: 'id', 'query', 'label', 'image' (first frame), 'frames' (all frames)\n",
    "        or None if processing fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract video path\n",
    "        video_rel_path = sample['video_path']\n",
    "        video_full_path = os.path.join(videos_base_path, os.path.basename(video_rel_path))\n",
    "\n",
    "        # Extract frames\n",
    "        support_frames = sample.get('support_frames', [])\n",
    "        frames = extract_frames_from_video(video_full_path, support_frames, max_frames)\n",
    "\n",
    "        if not frames:\n",
    "            return None\n",
    "\n",
    "        # Format question with choices (similar to ChartQA 'query')\n",
    "        query = format_question_with_choices(sample['question'], sample['choices'])\n",
    "\n",
    "        # Label is the answer (similar to ChartQA 'label' which is a list)\n",
    "        label = [sample['answer']]\n",
    "\n",
    "        return {\n",
    "            'id': sample['id'],\n",
    "            'query': query,  # Question + choices formatted\n",
    "            'label': label,  # Answer as list (ChartQA format)\n",
    "            'image': frames[0],  # First frame as main image\n",
    "            'frames': frames,  # All frames for video understanding\n",
    "            'video_path': video_rel_path,\n",
    "            'support_frames': support_frames,\n",
    "            'question': sample['question'],  # Keep original question\n",
    "            'choices': sample['choices'],  # Keep original choices\n",
    "            'answer': sample['answer']  # Keep original answer\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error processing sample {sample.get('id', 'unknown')}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"‚úÖ Processing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95f4dbd",
   "metadata": {},
   "source": [
    "## 7. Process All Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0215374a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1490 samples...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos:  37%|‚ñà‚ñà‚ñà‚ñã      | 553/1490 [07:51<13:42,  1.14it/s]"
     ]
    }
   ],
   "source": [
    "print(f\"Processing {total_samples} samples...\\n\")\n",
    "\n",
    "processed_data = []\n",
    "failed_count = 0\n",
    "\n",
    "for sample in tqdm(train_samples, desc=\"Processing videos\"):\n",
    "    processed = process_sample(sample, VIDEOS_PATH, max_frames=MAX_FRAMES)\n",
    "    if processed:\n",
    "        processed_data.append(processed)\n",
    "    else:\n",
    "        failed_count += 1\n",
    "\n",
    "print(f\"\\n‚úÖ Successfully processed: {len(processed_data)} samples\")\n",
    "print(f\"‚ùå Failed to process: {failed_count} samples\")\n",
    "print(f\"Success rate: {len(processed_data)/total_samples*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fc952a",
   "metadata": {},
   "source": [
    "## 8. Verify Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdb99e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample\n",
    "if processed_data:\n",
    "    sample = processed_data[0]\n",
    "    print(\"üìã Processed Sample Structure:\")\n",
    "    print(f\"ID: {sample['id']}\")\n",
    "    print(f\"\\nQuery (formatted question):\\n{sample['query']}\")\n",
    "    print(f\"\\nLabel (answer): {sample['label']}\")\n",
    "    print(f\"\\nNumber of frames: {len(sample['frames'])}\")\n",
    "    print(f\"Image size: {sample['image'].size}\")\n",
    "\n",
    "    # Display first frame\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(sample['image'])\n",
    "    plt.title(f\"Sample: {sample['id']}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8db372b",
   "metadata": {},
   "source": [
    "## 9. Format Data for Fine-tuning (ChartQA-style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd064bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data_for_training(sample, system_message=None):\n",
    "    \"\"\"\n",
    "    Format sample for Qwen2.5-VL fine-tuning with 4-bit quantization.\n",
    "    This creates the message structure needed for training.\n",
    "\n",
    "    Compatible with:\n",
    "    - Qwen2.5-VL-3B-Instruct\n",
    "    - 4-bit quantization (BitsAndBytes)\n",
    "    - LoRA fine-tuning\n",
    "    \"\"\"\n",
    "    if system_message is None:\n",
    "        system_message = (\n",
    "            \"B·∫°n l√† tr·ª£ l√Ω AI chuy√™n v·ªÅ an to√†n giao th√¥ng Vi·ªát Nam. \"\n",
    "            \"Nhi·ªám v·ª• c·ªßa b·∫°n l√† ph√¢n t√≠ch video dashcam v√† tr·∫£ l·ªùi c√°c c√¢u h·ªèi \"\n",
    "            \"v·ªÅ lu·∫≠t giao th√¥ng, bi·ªÉn b√°o, v√† t√¨nh hu·ªëng giao th√¥ng. \"\n",
    "            \"H√£y tr·∫£ l·ªùi ch√≠nh x√°c d·ª±a tr√™n n·ªôi dung video.\"\n",
    "        )\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": system_message}],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": sample[\"image\"]},  # Main image (first frame)\n",
    "                {\"type\": \"text\", \"text\": sample[\"query\"]},  # Formatted question\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": sample[\"label\"][0]}],  # Answer\n",
    "        },\n",
    "    ]\n",
    "\n",
    "\n",
    "# Format all processed data\n",
    "formatted_data = [format_data_for_training(sample) for sample in processed_data]\n",
    "\n",
    "print(f\"‚úÖ Formatted {len(formatted_data)} samples for Qwen2.5-VL training\")\n",
    "print(f\"üîß Compatible with 4-bit quantization + LoRA\")\n",
    "print(\"\\nüìã Sample formatted data:\")\n",
    "print(f\"Roles: {[msg['role'] for msg in formatted_data[0]]}\")\n",
    "print(f\"\\nSystem message: {formatted_data[0][0]['content'][0]['text'][:100]}...\")\n",
    "print(f\"\\nUser content types: {[c['type'] for c in formatted_data[0][1]['content']]}\")\n",
    "print(f\"\\nAssistant response: {formatted_data[0][2]['content'][0]['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bdcffe",
   "metadata": {},
   "source": [
    "## 10. Split Dataset (Train/Val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167dc94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Set seed for reproducibility\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Shuffle data\n",
    "indices = list(range(len(processed_data)))\n",
    "random.shuffle(indices)\n",
    "\n",
    "# Split indices\n",
    "split_idx = int(len(indices) * TRAIN_SPLIT)\n",
    "train_indices = indices[:split_idx]\n",
    "val_indices = indices[split_idx:]\n",
    "\n",
    "# Create train and validation sets\n",
    "train_data = [processed_data[i] for i in train_indices]\n",
    "val_data = [processed_data[i] for i in val_indices]\n",
    "\n",
    "# Formatted versions\n",
    "train_formatted = [formatted_data[i] for i in train_indices]\n",
    "val_formatted = [formatted_data[i] for i in val_indices]\n",
    "\n",
    "print(f\"‚úÖ Dataset split:\")\n",
    "print(f\"   Training samples: {len(train_data)} ({len(train_data)/len(processed_data)*100:.1f}%)\")\n",
    "print(f\"   Validation samples: {len(val_data)} ({len(val_data)/len(processed_data)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864d7f4e",
   "metadata": {},
   "source": [
    "## 11. Create HuggingFace Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1336d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset objects\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "\n",
    "# Create DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset\n",
    "})\n",
    "\n",
    "print(\"‚úÖ HuggingFace Dataset created:\")\n",
    "print(dataset_dict)\n",
    "print(\"\\nüìä Dataset features:\")\n",
    "print(train_dataset.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206b6e57",
   "metadata": {},
   "source": [
    "## 12. Save Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1770aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as JSON for easy inspection\n",
    "output_json = os.path.join(OUTPUT_DIR, \"traffic_mcq_dataset.json\")\n",
    "with open(output_json, 'w', encoding='utf-8') as f:\n",
    "    json.dump({\n",
    "        'train': [{\n",
    "            'id': s['id'],\n",
    "            'query': s['query'],\n",
    "            'label': s['label'],\n",
    "            'video_path': s['video_path'],\n",
    "            'question': s['question'],\n",
    "            'choices': s['choices'],\n",
    "            'answer': s['answer']\n",
    "        } for s in train_data],\n",
    "        'validation': [{\n",
    "            'id': s['id'],\n",
    "            'query': s['query'],\n",
    "            'label': s['label'],\n",
    "            'video_path': s['video_path'],\n",
    "            'question': s['question'],\n",
    "            'choices': s['choices'],\n",
    "            'answer': s['answer']\n",
    "        } for s in val_data]\n",
    "    }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Saved JSON dataset to: {output_json}\")\n",
    "\n",
    "# Save formatted data for training (pickle for preserving PIL Images)\n",
    "import pickle\n",
    "\n",
    "formatted_output = os.path.join(OUTPUT_DIR, \"formatted_training_data.pkl\")\n",
    "with open(formatted_output, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'train': train_formatted,\n",
    "        'validation': val_formatted\n",
    "    }, f)\n",
    "\n",
    "print(f\"‚úÖ Saved formatted training data to: {formatted_output}\")\n",
    "\n",
    "# Save dataset statistics\n",
    "stats = {\n",
    "    'total_samples': len(processed_data),\n",
    "    'train_samples': len(train_data),\n",
    "    'val_samples': len(val_data),\n",
    "    'failed_samples': failed_count,\n",
    "    'train_split': TRAIN_SPLIT,\n",
    "    'max_frames': MAX_FRAMES,\n",
    "    'random_seed': RANDOM_SEED\n",
    "}\n",
    "\n",
    "stats_file = os.path.join(OUTPUT_DIR, \"dataset_stats.json\")\n",
    "with open(stats_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(stats, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Saved dataset statistics to: {stats_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eea4fc",
   "metadata": {},
   "source": [
    "## 13. Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3041770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze answer distribution\n",
    "from collections import Counter\n",
    "\n",
    "# Extract answer options (A, B, C, D)\n",
    "answer_options = [sample['answer'][0] for sample in processed_data]\n",
    "answer_counts = Counter(answer_options)\n",
    "\n",
    "print(\"üìä Answer Distribution:\")\n",
    "for option, count in sorted(answer_counts.items()):\n",
    "    print(f\"   {option}: {count} ({count/len(processed_data)*100:.1f}%)\")\n",
    "\n",
    "# Analyze number of choices\n",
    "num_choices = [len(sample['choices']) for sample in processed_data]\n",
    "choice_counts = Counter(num_choices)\n",
    "\n",
    "print(\"\\nüìä Number of Choices Distribution:\")\n",
    "for num, count in sorted(choice_counts.items()):\n",
    "    print(f\"   {num} choices: {count} samples ({count/len(processed_data)*100:.1f}%)\")\n",
    "\n",
    "# Analyze frames extracted\n",
    "num_frames = [len(sample['frames']) for sample in processed_data]\n",
    "avg_frames = sum(num_frames) / len(num_frames)\n",
    "print(f\"\\nüìä Average frames per video: {avg_frames:.2f}\")\n",
    "print(f\"   Min frames: {min(num_frames)}\")\n",
    "print(f\"   Max frames: {max(num_frames)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0087fb9",
   "metadata": {},
   "source": [
    "## 14. Example: Load Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e87863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: How to load the formatted data for training\n",
    "import pickle\n",
    "\n",
    "# Load formatted data\n",
    "with open(formatted_output, 'rb') as f:\n",
    "    loaded_data = pickle.load(f)\n",
    "\n",
    "loaded_train = loaded_data['train']\n",
    "loaded_val = loaded_data['validation']\n",
    "\n",
    "print(f\"‚úÖ Loaded training data:\")\n",
    "print(f\"   Train: {len(loaded_train)} samples\")\n",
    "print(f\"   Validation: {len(loaded_val)} samples\")\n",
    "\n",
    "print(\"\\nüìã Sample loaded data structure:\")\n",
    "print(f\"Type: {type(loaded_train[0])}\")\n",
    "print(f\"Keys: {[msg['role'] for msg in loaded_train[0]]}\")\n",
    "print(f\"\\nFirst message:\")\n",
    "print(loaded_train[0][0])  # System message\n",
    "print(f\"\\nUser message content types:\")\n",
    "print([c['type'] for c in loaded_train[0][1]['content']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415f03f3",
   "metadata": {},
   "source": [
    "## 14.5. Test Dataset with Qwen2.5-VL (4-bit Quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68934d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Test loading Qwen2.5-VL model with 4-bit quantization\n",
    "# This verifies the dataset is compatible with the model\n",
    "\n",
    "print(\"üß™ Testing Qwen2.5-VL model loading with 4-bit quantization...\")\n",
    "\n",
    "try:\n",
    "    # Configure 4-bit quantization\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=USE_4BIT,\n",
    "        bnb_4bit_use_double_quant=BNB_4BIT_USE_DOUBLE_QUANT,\n",
    "        bnb_4bit_quant_type=BNB_4BIT_QUANT_TYPE,\n",
    "        bnb_4bit_compute_dtype=BNB_4BIT_COMPUTE_DTYPE,\n",
    "    )\n",
    "\n",
    "    print(f\"üì¶ Loading model: {MODEL_ID}\")\n",
    "    print(f\"‚öôÔ∏è Quantization config:\")\n",
    "    print(f\"   - 4-bit: {USE_4BIT}\")\n",
    "    print(f\"   - Compute dtype: {BNB_4BIT_COMPUTE_DTYPE}\")\n",
    "    print(f\"   - Quant type: {BNB_4BIT_QUANT_TYPE}\")\n",
    "    print(f\"   - Double quant: {BNB_4BIT_USE_DOUBLE_QUANT}\")\n",
    "\n",
    "    # Load processor\n",
    "    processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "    print(f\"‚úÖ Processor loaded\")\n",
    "\n",
    "    # Load model (uncomment to actually load)\n",
    "    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        quantization_config=bnb_config,\n",
    "        attn_implementation=\"flash_attention_2\",  # Optional: requires flash-attn\n",
    "    )\n",
    "    print(f\"‚úÖ Model loaded successfully!\")\n",
    "    print(f\"üìä Model parameters: {model.num_parameters():,}\")\n",
    "    print(f\"üíæ Memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "\n",
    "    print(\"\\nüí° To test the model, uncomment the model loading code above\")\n",
    "    print(\"‚ö†Ô∏è Note: Model loading requires ~3-4GB VRAM with 4-bit quantization\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"üí° Make sure you have installed: transformers, bitsandbytes, accelerate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bf79a9",
   "metadata": {},
   "source": [
    "## 15. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c06351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üìä DATASET PREPARATION COMPLETE - Qwen2.5-VL Ready\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n‚úÖ Total processed samples: {len(processed_data)}\")\n",
    "print(f\"‚úÖ Training samples: {len(train_data)}\")\n",
    "print(f\"‚úÖ Validation samples: {len(val_data)}\")\n",
    "print(f\"\\nüìÅ Output files:\")\n",
    "print(f\"   - JSON dataset: {output_json}\")\n",
    "print(f\"   - Formatted training data: {formatted_output}\")\n",
    "print(f\"   - Dataset statistics: {stats_file}\")\n",
    "print(f\"\\nüéØ Model Configuration:\")\n",
    "print(f\"   - Model: {MODEL_ID}\")\n",
    "print(f\"   - 4-bit Quantization: {USE_4BIT}\")\n",
    "print(f\"   - Compute dtype: {BNB_4BIT_COMPUTE_DTYPE}\")\n",
    "print(f\"   - Frames per video: {MAX_FRAMES}\")\n",
    "print(f\"\\nüí° Next steps:\")\n",
    "print(f\"   1. Load 'formatted_training_data.pkl' for fine-tuning\")\n",
    "print(f\"   2. Use BitsAndBytesConfig for 4-bit quantization\")\n",
    "print(f\"   3. Apply LoRA adapters (recommended for 4-bit training)\")\n",
    "print(f\"   4. Fine-tune with SFTTrainer from TRL\")\n",
    "print(f\"\\nüîó Compatible with:\")\n",
    "print(f\"   - Qwen2.5-VL-3B-Instruct (4-bit quantized)\")\n",
    "print(f\"   - LoRA fine-tuning with PEFT\")\n",
    "print(f\"   - TRL SFTTrainer\")\n",
    "print(f\"   - Flash Attention 2 (optional)\")\n",
    "print(f\"\\nüíæ Estimated VRAM usage:\")\n",
    "print(f\"   - Model (4-bit): ~3-4GB\")\n",
    "print(f\"   - Training (batch_size=1): ~6-8GB\")\n",
    "print(f\"   - Inference: ~4-5GB\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
